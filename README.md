# CSE 559: Algorithms in Computational Biology Final Project

**Authors:** Venkata Sai Gunda (1229553808), Akhil Routhu (1229582668),  
Ramesh Babu Mannam (1230786364), Siva Kumar Katta (1232008975),  
Vijetha Kasala (1231930780)

## Introduction

Predicting which T-cell receptors (TCRs) bind to which epitopes (peptide fragments) is crucial for understanding immune responses and developing effective immunotherapies and vaccines. In this project, we explore TCR-epitope binding prediction by leveraging TCR-BERT and catELMo embeddings.

We experimented with four main models:

1. **Baseline TCR-Bert Model**:  
   Uses CLS token embeddings from TCR-BERT for both TCRs and epitopes. These embeddings are concatenated and passed through a fully connected network to establish baseline performance.

2. **Cross-Attention TCR-Bert Model**:  
   Uses per-token embeddings from TCR-BERT and applies bi-directional cross-attention. The intuition is that the TCR queries the epitope, and the epitope queries the TCR, potentially capturing intricate sequence interactions.

3. **Baseline catELMo LSTM Model**:  
   A bidirectional LSTM-based model using catELMo embeddings, fine-tuned specifically for TCR and epitope sequences. This aims to effectively capture sequential relationships.

4. **catELMo-Transformer Model**:  
   A variant of the catELMo model that replaces LSTMs with transformer encoders to better model token-level dependencies and long-range interactions.

## Experimental Setup

**Data:**  
We used `TCREpitopePairs.csv`, which includes TCR sequences, epitope sequences, and binary binding labels.

**Embedding Generation:**
- **Baseline TCR-Bert Model**: CLS token embeddings from TCR-BERT.
- **Cross-Attention TCR-Bert Model**: Per-token embeddings from TCR-BERT with cross-attention.
- **Baseline catELMo Model**: catELMo embeddings generated by LSTMs.
- **catELMo-Transformer Model**: catELMo embeddings generated by transformer encoders.

**Data Splits:**
- **Random Split**: Train-test split done randomly.
- **TCR Split**: Test TCRs never appear in training.
- **EPI Split**: Test epitopes never appear in training.

**Metrics:**  
We computed Accuracy, Precision, Recall, F1-Score, and AUC over 5 runs to obtain averaged results for stable estimates.

**Hyperparameters (Selected):**

- **Baseline TCR-Bert Model:**
  - Activation: ReLU
  - Dropout: 0.5
  - Learning Rate: 1e-4
  - Hidden Layers: [1024, 512, 256]
  - Max Sequence Length: 50 (CLS embeddings)
  - Batch Size: 32
  - Epochs: 10

- **Cross-Attention TCR-Bert Model:**
  - Heads: 4
  - Linear Layers: [1024, 512]
  - Dropout: 0.25
  - Learning Rate: 1e-4
  - Max Sequence Length: 3 (chosen after trials)
  - Batch Size: 16
  - Epochs: 10

- **Baseline catELMo LSTM Model:**
  - LSTM Layers: 4
  - Hidden State Dim: 4096
  - Projection Dim: 1024
  - Dropout: 0.1
  - Batch Size: 128

- **catELMo-Transformer Model:**
  - Transformer Layers: 6
  - Heads: 8
  - Feed-Forward Dim: 2048
  - Dropout: 0.1
  - Batch Size: 128

## Results

### Baseline TCR-Bert Model Results (Averaged Over 5 Runs)

| Split  | Precision | Recall   | F1-Score | Accuracy | AUC     |
|--------|-----------|----------|----------|----------|---------|
| tcr    | 0.770920  | 0.626009 | 0.690371 | 0.720695 | 0.801577|
| epi    | 0.726388  | 0.562728 | 0.633745 | 0.675040 | 0.739110|
| random | 0.766363  | 0.632150 | 0.692210 | 0.719402 | 0.800171|

### Cross-Attention TCR-Bert Model Results (Averaged Over 5 Runs)

| Split  | Precision | Recall   | F1-Score | Accuracy | AUC     |
|--------|---------- |----------|----------|----------|---------|
| tcr    | 0.976473  | 0.074132 | 0.137801 | 0.538081 | 0.559846|
| epi    | 0.853305  | 0.014510 | 0.028534 | 0.506005 | 0.504402|
| random | 0.917487  | 0.083598 | 0.151402 | 0.535861 | 0.558465|


### CatELMo LSTM Model Results (Averaged Over 5 Runs) 

| Split  | AUC       | Accuracy | Precision| Recall   | F1-Score|
|--------|---------- |----------|----------|----------|---------|
| tcr    | 0.99024   | 0.9429   | 0.97594  | 0.90798  | 0.94068 |
| epi    | 0.97794   | 0.9344   | 0.95392  | 0.9135   | 0.93396 |

### CatELMo Transformer Model Results (Averaged Over 5 Runs) 

| Split  | AUC       | Accuracy | Precision| Recall   | F1-Score|
|--------|---------- |----------|----------|----------|---------|
| TCR    | 0.9892    | 0.9446   | 0.96868  | 0.9188   | 0.9430  |
| EPI    | 0.96716   | 0.88404  | 0.84254  | 0.9454   | 0.89002 |


## Repository Structure

```bash
.
├── base_tcr_bert.py              # Baseline TCR-Bert model code
├── cross_attention_tcr_bert.py   # Cross-attention TCR-Bert model code
├── base_catELMo_model.py         # Baseline catELMo LSTM model code
├── base_catELMo_transformer.py   # catELMo-Transformer model code
├── run.sh                        # SLURM script to run models on HPC
├── environment.yml               # Environment file
└── data/
    └── TCREpitopePairs.csv       # The dataset (not included, must be downloaded)

```
## Setup

### Conda Environment

Create and activate the conda environment:

```bash
conda env create -f environment.yml
conda activate environment
```

Ensure that the environment has all the required dependencies.

